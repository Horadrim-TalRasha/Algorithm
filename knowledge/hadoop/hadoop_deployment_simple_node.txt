ubuntu 16.04 部署 hadoop

1. 创建hadoop用户
sudo useradd -m hadoop -s /bin/bash

2. set hadoop用户密码
sudo passwd hadoop

3. 增加管理员权限
sudo adduser hadoop sudo

4. 安装ssh，配置ssh私钥(skip)

5. 安装JAVA并设置环境变量(java 7废弃，安装java8)
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

6. 下载hadoop，解压并移动到指定目录
wget http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.6.5/hadoop-2.6.5.tar.gz
tar -zxf /my_path/hadoop-2.6.5.tar.gz
mv ...........
chown -R hadoop ./hadoop

7. 单机配置
cd /usr/local/hadoop
mkdir ./input



IF A
1. 单节点部署

在.bashrc中添加全局变量
#HADOOP VARIABLES START
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
export HADOOP_INSTALL=/usr/local/hadoop
export PATH=$PATH:$HADOOP_INSTALL/bin
export PATH=$PATH:$HADOOP_INSTALL/sbin
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL
export YARN_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib"
#HADOOP VARIABLES END

2. /usr/local/hadoop/etc/hadoop/hadoop-env.sh中添加JAVA_HOME(can not skip)
添加"export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64"

3. 配置/usr/local/hadoop/etc/hadoop/mapred-site.xml
<property>
  <name>hadoop.tmp.dir</name>
  <value>/app/hadoop/tmp</value>
  <description>A base for other temporary directories.</description>
 </property>

 <property>
  <name>fs.default.name</name>
  <value>hdfs://localhost:54320</value>
  <description>The name of the default file system.  A URI whose
  scheme and authority determine the FileSystem implementation.  The
  uri's scheme determines the config property (fs.SCHEME.impl) naming
  the FileSystem implementation class.  The uri's authority is used to
  determine the host, port, etc. for a filesystem.</description>
 </property>
mkdir -p /app/hadoop/tmp
sudo chown hadoop:hadoop /app/hadoop/tmp


4. 配置/usr/local/hadoop/etc/hadoop/mapred-site.xml
<property>
  <name>mapred.job.tracker</name>
  <value>localhost:54321</value>
  <description>The host and port that the MapReduce job tracker runs
  at.  If "local", then jobs are run in-process as a single map
  and reduce task.
  </description>
 </property>

5. 配置/usr/local/hadoop/etc/hadoop/hdfs-site.xml
<property>
  <name>dfs.replication</name>
  <value>1</value>
  <description>Default block replication.
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
  </description>
 </property>
 <property>
   <name>dfs.namenode.name.dir</name>
   <value>file:/usr/local/hadoop_store/hdfs/namenode</value>
 </property>
 <property>
   <name>dfs.datanode.data.dir</name>
   <value>file:/usr/local/hadoop_store/hdfs/datanode</value>
 </property>

mkdir -p /usr/local/hadoop_store/hdfs/namenode
mkdir -p /usr/local/hadoop_store/hdfs/datanode
chown -R hadoop:hadoop /usr/local/hadoop_store

6. 格式化Hadoop文件系统
hadoop namenode -format

7. 启动hadoop
/usr/local/hadoop/sbin/startup-all.sh

8. 验证hadoop进程
/usr/local/hadoop/sbin/jps